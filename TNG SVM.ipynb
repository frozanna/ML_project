{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "import statistics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, average_precision_score\n",
    "from random import sample, randint\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start_time = default_timer()\n",
    "\n",
    "    class _Timer():\n",
    "      start = start_time\n",
    "      end = default_timer()\n",
    "      duration = end - start\n",
    "\n",
    "    yield _Timer\n",
    "\n",
    "    end_time = default_timer()\n",
    "    _Timer.end = end_time\n",
    "    _Timer.duration = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "y_train = newsgroups_train.target\n",
    "x_test = vectorizer.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target\n",
    "classes_count = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change dimensions to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=500)\n",
    "svd.fit(x_train)\n",
    "svd_x_train = svd.transform(x_train)\n",
    "svd_x_test = svd.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_res, y_train_res = resample(svd_x_train, y_train, n_samples=5000, replace=False, random_state=0)\n",
    "#x_test_res, y_test_res = resample(svd_x_test, y_test, n_samples=500, replace=False, random_state=0)\n",
    "#x_train_res = x_train_res.reshape((x_train_res.shape[0],-1))\n",
    "#x_test_res = x_test_res.reshape((x_test_res.shape[0],-1))\n",
    "#y_train_res = y_train_res.reshape((y_train_res.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler(with_mean=False).fit(x_train)\n",
    "#x_train_scaled = scaler.transform(x_train) # scaling data\n",
    "#x_test_scaled = scaler.transform(x_test)\n",
    "#print(x_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM on full data and full features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(probability=True)\n",
    "clf.fit(svd_x_train, y_train)\n",
    "#clf.fit(x_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(svd_x_test)\n",
    "#y_pred = clf.predict(x_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#accuracy = accuracy_score(y_test_res, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7895645246946362"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy #0.78956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7906544567289585"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_score = f1_score(y_test, y_pred, average='weighted')\n",
    "#f_score = f1_score(y_test_res, y_pred, average='weighted')\n",
    "f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 sub-classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = [0.1, 0.25]\n",
    "n_features = [0.25, 0.5, 0.75]\n",
    "\n",
    "samples_all = svd_x_train.shape[0]\n",
    "samples_all_test = svd_x_test.shape[0]\n",
    "features_all = svd_x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minor_classifiers(samples_perc, features_perc):\n",
    "    feature_list = [n for n in range(500)]\n",
    "    classifiers = []\n",
    "    \n",
    "    for samples in samples_perc:\n",
    "        for features in features_perc:\n",
    "            f = sample(feature_list, int(features_all * features))\n",
    "            x_train_f = svd_x_train[:,f]\n",
    "            x_test_f = svd_x_test[:,f]\n",
    "            \n",
    "            x_train_s, y_train_s = resample(x_train_f, y_train, n_samples=int(samples * samples_all), replace=False, random_state=0)\n",
    "            #x_test_s, y_test_s = resample(svd_x_test, y_test, n_samples=int(samples * samples_all_test), replace=False, random_state=0)\n",
    "            \n",
    "            svm_clf = SVC(probability=True)\n",
    "            svm_clf.fit(x_train_s, y_train_s)\n",
    "            \n",
    "            y_pred = svm_clf.predict(x_test_f)\n",
    "            pred = svm_clf.predict_proba(x_test_f)\n",
    "            \n",
    "            classifiers.append((y_pred, pred))\n",
    "            \n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas = get_minor_classifiers(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pred(predictions):\n",
    "    #predictions = list of tuples \n",
    "    m = len(predictions[0][0])\n",
    "    all_results = [[0 for x in range(classes_count)] for y in range(m)] \n",
    "    results = [0] * m\n",
    "    for (_, pred_proba) in predictions:\n",
    "        for i in range(m):\n",
    "            for j in range(classes_count):\n",
    "                all_results[i][j] += pred_proba[i][j]\n",
    "    for i in range(m):\n",
    "        results[i] = all_results[i].index(max(all_results[i]))\n",
    "    return results\n",
    "\n",
    "def majority_pred(predictions):\n",
    "    m = len(predictions[0][0])\n",
    "    results = [0] * m\n",
    "    for i in range(m):\n",
    "        all_results = [0 for x in range(classes_count)] \n",
    "        for (pred, _) in predictions:\n",
    "            all_results[pred[i]] += 1\n",
    "        results[i] = all_results.index(max(all_results))        \n",
    "    return results\n",
    "\n",
    "def borda_pred(predictions):\n",
    "    m = len(predictions[0][0])\n",
    "    all_results = [[0 for x in range(classes_count)] for y in range(m)] \n",
    "    results = [0] * m\n",
    "    \n",
    "    def get_final_borda_points(predictions):\n",
    "        return np.argsort(np.argsort(predictions)).tolist()\n",
    "\n",
    "    for (_, pred_proba) in predictions:\n",
    "        for i in range(m):\n",
    "            pred_proba[i] = get_final_borda_points(pred_proba[i])\n",
    "    for (_, pred_proba) in predictions:\n",
    "        for i in range(m):\n",
    "            for j in range(classes_count):\n",
    "                all_results[i][j] += pred_proba[i][j]\n",
    "    for i in range(m):\n",
    "        results[i] = all_results[i].index(max(all_results[i]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = majority_pred(clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6768454593733404"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise_label(labels, percent):\n",
    "    labels_with_noise = deepcopy(labels)\n",
    "    arr_size = labels.shape[0]\n",
    "    indexes = [n for n in range(arr_size)]\n",
    "    indexes_to_change = sample(indexes, int(arr_size * percent))\n",
    "    \n",
    "    for i in indexes_to_change:\n",
    "        old_val = labels[i]\n",
    "        new_val = randint(0, classes_count)\n",
    "        while old_val == new_val:\n",
    "            new_val = randint(0, classes_count)\n",
    "        labels_with_noise[i] = new_val\n",
    "        \n",
    "    return labels_with_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 111.54180649999944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\majer\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_100iter = SVC(probability=True, max_iter=50) #221 dla 100, 24 dla 10, 46 dla 20, 112 dla 50\n",
    "\n",
    "with elapsed_timer() as t:\n",
    "    clf_100iter.fit(svd_x_train, y_train)\n",
    "\n",
    "print(\"duration: \" + str(t.duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki jakości klasyfikatorów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87538665, 0.87627044, 0.88201502, 0.87892179, 0.877542  ])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UWAGA NIE PUSZCZAĆ BEZ POWODU - linijka niezakomentowana żeby uniknąć restartowania kernela\n",
    "#cross-validation -> wybór metody fuzji decyzji?\n",
    "scores = cross_validate(clf, svd_x_train, y_train, cv=5, scoring=('accuracy', 'neg_log_loss')) #zrobić klasę Model, robiącą model z 10 klasyfikatorów\n",
    "print(scores['test_accuracy'])\n",
    "print(scores['test_neg_log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.79\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(svd_x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score: {0:0.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss score: 0.72\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = clf.predict_proba(svd_x_test)\n",
    "loss = log_loss(y_test, y_pred_proba)\n",
    "print('Loss score: {0:0.2f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-8aaf108fd516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maverage_precision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Precision-recall score: {0:0.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_precision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36maverage_precision_score\u001b[1;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    213\u001b[0m                                 pos_label=pos_label)\n\u001b[0;32m    214\u001b[0m     return _average_binary_score(average_precision, y_true, y_score,\n\u001b[1;32m--> 215\u001b[1;33m                                  average, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0my_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multilabel-indicator\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} format is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(y_test, y_pred, pos_label=1)\n",
    "print('Precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
